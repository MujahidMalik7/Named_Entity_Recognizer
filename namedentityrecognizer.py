# -*- coding: utf-8 -*-
"""NamedEntityRecognizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1X1GqZ1UX1y5GSizEuMq6vDpDNg4Jwi6j
"""

pip install spacy datasets seqeval spacy-lookups-data

!python -m spacy download en_core_web_sm
!python -m spacy download en_core_web_trf

from google.colab import drive
drive.mount('/content/drive')

data_folder = '/content/drive/MyDrive/Named_Entity_Recognition/'
test_file = data_folder + 'test.txt'

def read_conll_data(filepath):
    sentences = []
    labels = []
    with open(filepath, 'r', encoding='utf-8') as f:
        sentence = []
        label_seq = []
        for line in f:
            line = line.strip()
            if not line:
                if sentence:
                    sentences.append(sentence)
                    labels.append(label_seq)
                    sentence = []
                    label_seq = []
            else:
                splits = line.split()
                if len(splits) < 2:
                    continue
                word = splits[0]
                ner_tag = splits[-1]
                sentence.append(word)
                label_seq.append(ner_tag)
    return sentences, labels

sentences, true_labels = read_conll_data('/content/drive/MyDrive/Named_Entity_Recognition/test.txt')

for i, (tokens, labels) in enumerate(zip(sentences, true_labels)):
    if len(tokens) != len(labels):
        print(f"Mismatch in sentence {i}: {len(tokens)} tokens vs {len(labels)} labels")

import spacy
from seqeval.metrics import precision_score, recall_score, f1_score

def get_spacy_bio_tags(doc, label_map):
    tags = ["O"] * len(doc)
    for ent in doc.ents:
        start = ent.start
        end = ent.end
        label = label_map.get(ent.label_, "O")
        if label == "O":
            continue
        tags[start] = "B-" + label
        for i in range(start + 1, end):
            tags[i] = "I-" + label
    return tags

def evaluate_spacy_model_aligned(model_name, sentences, true_labels):
    nlp = spacy.load(model_name)
    spacy2conll = {
        "PERSON": "PER",
        "NORP": "MISC",
        "ORG": "ORG",
        "GPE": "LOC",
        "LOC": "LOC",
        "FAC": "LOC",
        "MISC": "MISC"
    }

    pred_labels = []
    for sent, true_label in zip(sentences, true_labels):
        # Create a Doc object with the input tokens
        doc = spacy.tokens.Doc(nlp.vocab, words=sent)

        # Process the Doc with the full pipeline (transformer or tok2vec + ner)
        doc = nlp(doc)  # This runs the entire pipeline, including transformer/ner

        tags = get_spacy_bio_tags(doc, spacy2conll)

        if len(tags) != len(true_label):
            print(f"Length mismatch in {model_name}: predicted {len(tags)}, true {len(true_label)}")
            if len(tags) > len(true_label):
                tags = tags[:len(true_label)]
            else:
                tags += ["O"] * (len(true_label) - len(tags))

        pred_labels.append(tags)

    precision = precision_score(true_labels, pred_labels)
    recall = recall_score(true_labels, pred_labels)
    f1 = f1_score(true_labels, pred_labels)

    print(f"=== {model_name} Overall Scores ===")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1 Score:  {f1:.4f}\n")

# Run evaluation
evaluate_spacy_model_aligned("en_core_web_sm", sentences, true_labels)
evaluate_spacy_model_aligned("en_core_web_trf", sentences, true_labels)

from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline
from seqeval.metrics import precision_score, recall_score, f1_score
import torch

# Load model and tokenizer
model_name = "dslim/bert-base-NER"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)

ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

def predict_bert_ner(sentences, true_labels):
    pred_labels = []

    for sent, gold in zip(sentences, true_labels):
        text = " ".join(sent)
        preds = ner_pipeline(text)

        # Build token-level BIO tags
        tags = ["O"] * len(sent)

        for entity in preds:
            word = entity['word']
            entity_label = entity['entity_group']
            start = entity['start']
            end = entity['end']
            entity_text = entity['word']

            entity_tokens = tokenizer.tokenize(entity['word'])
            entity_len = len(entity_text.split())

            # Map detected entity back to original token sequence
            try:
                entity_words = entity['word'].split()
                for i, token in enumerate(sent):
                    if sent[i:i+entity_len] == entity_words:
                        tags[i] = f"B-{entity_label}"
                        for j in range(1, entity_len):
                            tags[i+j] = f"I-{entity_label}"
                        break
            except:
                continue

        if len(tags) != len(gold):
            print(f"Warning: Length mismatch, padding with O. Sent: {len(sent)} vs tags: {len(tags)}")
            tags += ["O"] * (len(gold) - len(tags))

        pred_labels.append(tags)

    precision = precision_score(true_labels, pred_labels)
    recall = recall_score(true_labels, pred_labels)
    f1 = f1_score(true_labels, pred_labels)

    print(f"=== dslim/bert-base-NER Overall Scores ===")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1 Score:  {f1:.4f}\n")

predict_bert_ner(sentences, true_labels)

import spacy
from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline

#  Custom sentences
custom_sentences = [
    "Barack Obama was elected President of the United States in 2008.",
    "Apple launched the iPhone 15 Pro Max at its Cupertino headquarters in September 2023.",
    "The Eiffel Tower in Paris attracts millions of tourists every year.",
    "Microsoft announced a $68.7 billion acquisition of Activision Blizzard on January 18, 2022.",
    "J.K. Rowling’s book 'Harry Potter and the Philosopher's Stone' was published by Bloomsbury in 1997."
]

#  Load spaCy models
nlp_sm = spacy.load("en_core_web_sm")
nlp_trf = spacy.load("en_core_web_trf")

#  Load transformer model (dslim/bert-base-NER)
tokenizer = AutoTokenizer.from_pretrained("dslim/bert-base-NER")
model = AutoModelForTokenClassification.from_pretrained("dslim/bert-base-NER")
bert_ner = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

#  Helper function to print results
def print_ner_results(model_name, doc_or_output):
    print(f"\n=== {model_name} ===")
    if model_name.startswith("en_core"):
        for sent in doc_or_output:
            print(f"\n→ {sent.text}")
            for ent in sent.ents:
                print(f"   - {ent.text} ({ent.label_})")
    else:  # transformer
        for i, output in enumerate(doc_or_output):
            print(f"\n→ {custom_sentences[i]}")
            for ent in output:
                print(f"   - {ent['word']} ({ent['entity_group']})")

#  Run inference for all models
doc_sm = list(nlp_sm.pipe(custom_sentences))
doc_trf = list(nlp_trf.pipe(custom_sentences))
bert_outputs = [bert_ner(text) for text in custom_sentences]

#  Print results
print_ner_results("en_core_web_sm", doc_sm)
print_ner_results("en_core_web_trf", doc_trf)
print_ner_results("dslim/bert-base-NER", bert_outputs)

import matplotlib.pyplot as plt
import numpy as np

# Model names
models = ['en_core_web_sm', 'en_core_web_trf', 'dslim/bert-base-NER']

# Your evaluation scores
precision = [0.6712, 0.8106, 0.9172]
recall = [0.5363, 0.7041, 0.7176]
f1 = [0.5962, 0.7536, 0.8052]

x = np.arange(len(models))  # label locations
width = 0.25  # bar width

fig, ax = plt.subplots(figsize=(10,6))

# Plot bars
rects1 = ax.bar(x - width, precision, width, label='Precision')
rects2 = ax.bar(x, recall, width, label='Recall')
rects3 = ax.bar(x + width, f1, width, label='F1 Score')

# Add some text for labels, title and axes ticks
ax.set_ylabel('Scores')
ax.set_title('NER Model Evaluation Comparison')
ax.set_xticks(x)
ax.set_xticklabels(models)
ax.set_ylim(0, 1)
ax.legend()

# Show values on top of bars
def autolabel(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate(f'{height:.3f}',
                    xy=(rect.get_x() + rect.get_width()/2, height),
                    xytext=(0,3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

autolabel(rects1)
autolabel(rects2)
autolabel(rects3)

plt.show()

import random

def generate_diverse_ner_data_50():
    persons = [["Barack", "Obama"], ["Elon", "Musk"], ["J.K.", "Rowling"], ["Serena", "Williams"], ["Albert", "Einstein"]]
    orgs = ["Google", "Microsoft", "Apple", "Tesla", "NASA", "United Nations"]
    locs = ["Paris", "New York", "Hawaii", "London", "California", "Tokyo", "Berlin", "Dubai"]
    gpes = ["USA", "France", "Germany", "Japan", "Canada", "Brazil"]
    dates = ["2023", "1997", "January", "December", "August", "July", "18th", "2020"]
    money = ["$100", "$68.7 billion", "$5 million", "€50"]
    misc = ["Python", "Windows", "C++", "COVID-19", "World Cup"]
    products = ["iPhone", "Tesla Model S", "PlayStation", "Galaxy S21", "Xbox"]
    events = ["Olympics", "Tech Summit", "World Cup", "Comic-Con"]
    facilities = ["Eiffel Tower", "Golden Gate Bridge", "Statue of Liberty", "Empire State Building"]

    sentences = []
    labels = []

    # 50 diverse sentences templates
    templates = [
        lambda: random.choice(persons) + ["was", "born", "in"] + [random.choice(locs)] + ["in", random.choice(dates), "."],
        lambda: ["The", random.choice(orgs), "announced", "a", random.choice(money), "investment", "on", random.choice(dates), "."],
        lambda: ["Many", "visited", random.choice(facilities), "during", random.choice(events), "."],
        lambda: ["I", "met", random.choice(persons)[0], "at", random.choice(orgs), "headquarters", "in", random.choice(gpes), "."],
        lambda: [random.choice(products), "sales", "surged", "in", random.choice(gpes), "in", random.choice(dates), "."],
        lambda: random.choice(persons) + ["won", "the", random.choice(events), "in", random.choice(dates), "in", random.choice(gpes), "."],
        lambda: [random.choice(misc), "is", "popular", "among", "developers", "."],
        lambda: ["The", random.choice(facilities), "is", "located", "in", random.choice(locs), "."],
        lambda: ["The", random.choice(orgs), "released", random.choice(products), "in", random.choice(dates), "."],
        lambda: ["The", random.choice(events), "will", "be", "held", "in", random.choice(gpes), "."]
    ]

    def label_entity(tokens, label):
        return ["B-" + label] + ["I-" + label] * (len(tokens) - 1)

    for _ in range(50):
        sent_tokens = []
        sent_labels = []

        # pick a random template sentence generator
        template_fn = random.choice(templates)
        sent = template_fn()

        i = 0
        while i < len(sent):
            matched = False
            # check for multi-token entities first (PERSON and FACILITIES are lists)
            # PERSON check
            for person in persons:
                length = len(person)
                if sent[i:i+length] == person:
                    sent_labels.extend(label_entity(person, "PER"))
                    sent_tokens.extend(person)
                    i += length
                    matched = True
                    break
            if matched:
                continue

            # FACILITY check
            for fac in facilities:
                fac_tokens = fac.split()
                length = len(fac_tokens)
                if sent[i:i+length] == fac_tokens:
                    sent_labels.extend(label_entity(fac_tokens, "FAC"))
                    sent_tokens.extend(fac_tokens)
                    i += length
                    matched = True
                    break
            if matched:
                continue

            # Single token entities
            token = sent[i]

            if token in orgs:
                sent_labels.append("B-ORG")
                sent_tokens.append(token)
            elif token in locs:
                sent_labels.append("B-LOC")
                sent_tokens.append(token)
            elif token in gpes:
                sent_labels.append("B-GPE")
                sent_tokens.append(token)
            elif token in dates:
                sent_labels.append("B-DATE")
                sent_tokens.append(token)
            elif token in money:
                sent_labels.append("B-MONEY")
                sent_tokens.append(token)
            elif token in misc:
                sent_labels.append("B-MISC")
                sent_tokens.append(token)
            elif token in products:
                sent_labels.append("B-PROD")
                sent_tokens.append(token)
            elif token in events:
                sent_labels.append("B-EVENT")
                sent_tokens.append(token)
            else:
                sent_labels.append("O")
                sent_tokens.append(token)
            i += 1

        sentences.append(sent_tokens)
        labels.append(sent_labels)

    return sentences, labels

# Generate the dataset
sentences_50, labels_50 = generate_diverse_ner_data_50()

# Print an example
for i in range(3):
    print("Sentence:", sentences_50[i])
    print("Labels:  ", labels_50[i])
    print()

import spacy
from seqeval.metrics import precision_score, recall_score, f1_score

def get_spacy_bio_tags(doc, label_map):
    tags = ["O"] * len(doc)
    for ent in doc.ents:
        start = ent.start
        end = ent.end
        label = label_map.get(ent.label_, "O")
        if label == "O":
            continue
        tags[start] = "B-" + label
        for i in range(start + 1, end):
            tags[i] = "I-" + label
    return tags

def evaluate_spacy_model_aligned(model_name, sentences, true_labels):
    nlp = spacy.load(model_name)
    spacy2conll = {
        "PERSON": "PER",
        "NORP": "MISC",
        "ORG": "ORG",
        "GPE": "GPE",
        "LOC": "LOC",
        "FAC": "FAC",
        "MISC": "MISC",
        "PRODUCT": "PROD",
        "EVENT": "EVENT",
        "DATE": "DATE",
        "MONEY": "MONEY"
    }

    pred_labels = []
    for sent, true_label in zip(sentences, true_labels):
        doc = spacy.tokens.Doc(nlp.vocab, words=sent)
        doc = nlp(doc)
        tags = get_spacy_bio_tags(doc, spacy2conll)

        # Fix length mismatch if any
        if len(tags) != len(true_label):
            if len(tags) > len(true_label):
                tags = tags[:len(true_label)]
            else:
                tags += ["O"] * (len(true_label) - len(tags))

        pred_labels.append(tags)

    precision = precision_score(true_labels, pred_labels)
    recall = recall_score(true_labels, pred_labels)
    f1 = f1_score(true_labels, pred_labels)

    print(f"=== {model_name} Overall Scores ===")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1 Score:  {f1:.4f}\n")



from transformers import AutoTokenizer, AutoModelForTokenClassification
from transformers import pipeline
from seqeval.metrics import precision_score, recall_score, f1_score

# Load tokenizer and model
model_name = "dslim/bert-base-NER"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForTokenClassification.from_pretrained(model_name)

# Hugging Face NER pipeline
ner_pipeline = pipeline("ner", model=model, tokenizer=tokenizer, aggregation_strategy="simple")

def convert_bio_to_standard(labels):
    # Converts BIO tags to standard labels (just the entity label or "O")
    standard_labels = []
    for label_seq in labels:
        seq = []
        for tag in label_seq:
            if tag == "O":
                seq.append("O")
            else:
                # Remove B- or I- prefix
                seq.append(tag.split("-", maxsplit=1)[-1])
        standard_labels.append(seq)
    return standard_labels

def get_preds_for_sentence(sentence):
    # Run the HF NER pipeline on the sentence (list of tokens)
    text = " ".join(sentence)
    ents = ner_pipeline(text)

    # Initialize all tags as "O"
    preds = ["O"] * len(sentence)

    # Map entity spans to token indices
    # This is approximate since HF pipeline uses word-level offsets,
    # but for testing, this should suffice
    for ent in ents:
        start_char = ent['start']
        end_char = ent['end']
        label = ent['entity_group']

        # Map char offsets to token indices
        char_count = 0
        start_token_idx = None
        end_token_idx = None
        for i, token in enumerate(sentence):
            token_start = char_count
            token_end = char_count + len(token)
            if start_token_idx is None and token_start <= start_char < token_end:
                start_token_idx = i
            if token_start < end_char <= token_end:
                end_token_idx = i
                break
            char_count += len(token) + 1  # plus space

        if start_token_idx is None:
            continue
        if end_token_idx is None:
            end_token_idx = start_token_idx

        preds[start_token_idx] = "B-" + label
        for idx in range(start_token_idx + 1, end_token_idx + 1):
            preds[idx] = "I-" + label
    return preds

def evaluate_bert_model(sentences, true_labels):
    pred_labels = []
    true_labels_std = convert_bio_to_standard(true_labels)

    for sent, true_lab in zip(sentences, true_labels):
        preds = get_preds_for_sentence(sent)
        # Align length with true labels if mismatch
        if len(preds) > len(true_lab):
            preds = preds[:len(true_lab)]
        elif len(preds) < len(true_lab):
            preds += ["O"] * (len(true_lab) - len(preds))
        pred_labels.append(preds)

    # Convert predicted labels to standard format (no B-/I- prefix)
    pred_labels_std = convert_bio_to_standard(pred_labels)

    precision = precision_score(true_labels_std, pred_labels_std)
    recall = recall_score(true_labels_std, pred_labels_std)
    f1 = f1_score(true_labels_std, pred_labels_std)

    print(f"=== {model_name} Overall Scores ===")
    print(f"Precision: {precision:.4f}")
    print(f"Recall:    {recall:.4f}")
    print(f"F1 Score:  {f1:.4f}\n")

evaluate_spacy_model_aligned("en_core_web_sm", sentences_50, labels_50)
evaluate_spacy_model_aligned("en_core_web_trf", sentences_50, labels_50)
evaluate_bert_model(sentences_50, labels_50)

import matplotlib.pyplot as plt
import numpy as np

# Scores from your evaluation
models = ['en_core_web_sm', 'en_core_web_trf', 'dslim/bert-base-NER']
precision_scores = [0.8000, 0.7414, 0.3140]
recall_scores = [0.6557, 0.7049, 0.2455]
f1_scores = [0.7207, 0.7227, 0.2755]

x = np.arange(len(models))  # label locations
width = 0.25  # bar width

fig, ax = plt.subplots(figsize=(10, 6))

# Plot bars
rects1 = ax.bar(x - width, precision_scores, width, label='Precision', color='skyblue')
rects2 = ax.bar(x, recall_scores, width, label='Recall', color='lightgreen')
rects3 = ax.bar(x + width, f1_scores, width, label='F1 Score', color='salmon')

# Labels and titles
ax.set_ylabel('Scores')
ax.set_ylim(0, 1)
ax.set_title('Comparison of NER Models on Custom Dataset (50 sentences)')
ax.set_xticks(x)
ax.set_xticklabels(models)
ax.legend()

# Add score labels on top of bars
def add_labels(rects):
    for rect in rects:
        height = rect.get_height()
        ax.annotate(f'{height:.2f}',
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

add_labels(rects1)
add_labels(rects2)
add_labels(rects3)

plt.show()

!pip install spacy[transformers]

!python -m spacy download en_core_web_trf

pip install spacy datasets seqeval spacy-lookups-data

import spacy
from IPython.display import display, HTML

# Load once
nlp = spacy.load("en_core_web_trf")

# Highlight function
def highlight_entities_html(doc):
    html = ""
    last_idx = 0
    colors = {
        "PERSON": "lightgreen",
        "ORG": "lightblue",
        "GPE": "orange",
        "DATE": "pink",
        "MONEY": "yellow",
        "PRODUCT": "violet",
        "LOC": "lightcoral",
        "MISC": "lightgrey",
        "EVENT": "khaki",
        "LAW": "lightcyan",
        "TIME": "plum",
        "PERCENT": "lightyellow"
    }
    for ent in doc.ents:
        html += doc.text[last_idx:ent.start_char]
        color = colors.get(ent.label_, "lightgrey")
        html += f'<span style="background-color:{color}; font-weight:bold;">{ent.text} ({ent.label_})</span>'
        last_idx = ent.end_char
    html += doc.text[last_idx:]
    return html

print("Model ready. Type your sentence and press Enter (type 'x' to exit):")

# Single continuous loop until exit
while True:
    text = input("\nEnter sentence: ").strip()

    if text.lower() in ["x", "exit"]:
        print("Exiting.")
        break

    if not text:
        print("⚠️ No input. Try again.")
        continue

    doc = nlp(text)
    print(f"\n→ Entities in: \"{text}\"\n")
    if doc.ents:
        display(HTML(highlight_entities_html(doc)))
    else:
        print("No entities found.")